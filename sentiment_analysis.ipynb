{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "hh4aCm-XQLCH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4IBaFnfPSWW",
        "outputId": "ba50d9d8-a88f-4c9a-e622-c42e71dcffa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from gensim.models import Word2Vec\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "K1vEB4U-QS0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/ProyekAnalisisSentimen/dataset.csv')"
      ],
      "metadata": {
        "id": "W4DiAG5CQWlA"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download stopwords and tokenizer"
      ],
      "metadata": {
        "id": "IRQF-Zc-XBr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKOqAnjyXEi2",
        "outputId": "0ff3d890-5271-41d0-b61b-5322a74ac159"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define stopwords"
      ],
      "metadata": {
        "id": "-b_0DHCNXK7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('indonesian'))"
      ],
      "metadata": {
        "id": "ADhCq3RsXL5g"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Lexicon"
      ],
      "metadata": {
        "id": "CYJDOMCGXt7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = {\"bagus\", \"mantap\", \"puas\", \"baik\", \"hebat\", \"luar biasa\", \"menyenangkan\"}\n",
        "negative_words = {\"buruk\", \"jelek\", \"kecewa\", \"tidak puas\", \"parah\", \"menyedihkan\", \"payah\"}\n"
      ],
      "metadata": {
        "id": "baQgV_qdXu-K"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for cleaning text"
      ],
      "metadata": {
        "id": "iPtNpQArR97x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "1fg8KVLYSOOs"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to determine sentiment"
      ],
      "metadata": {
        "id": "OwahF6GNLh8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_sentiment(text):\n",
        "    words = set(text.split())\n",
        "    pos_count = len(words & positive_words)\n",
        "    neg_count = len(words & negative_words)\n",
        "    if pos_count > neg_count:\n",
        "        return \"positive\"\n",
        "    elif neg_count > pos_count:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\""
      ],
      "metadata": {
        "id": "HaWUD36lLjH2"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply preprocessing"
      ],
      "metadata": {
        "id": "e5h8L_yXTHN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"clean_text\"] = df[\"review\"].astype(str).apply(clean_text)\n",
        "df[\"sentiment\"] = df[\"clean_text\"].apply(label_sentiment)"
      ],
      "metadata": {
        "id": "00dONJDNTKD3"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop empty rows"
      ],
      "metadata": {
        "id": "ALFIKTeMcTq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"clean_text\"].str.strip() != \"\"]"
      ],
      "metadata": {
        "id": "zYaxf2bAcUoe"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data"
      ],
      "metadata": {
        "id": "2bG7r29RT4sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "oSqT02qqT7wS"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle class imbalance"
      ],
      "metadata": {
        "id": "d-hQxqY4MQwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan jumlah data sebelum proses\n",
        "assert len(X_train) == len(y_train), f\"Jumlah tidak sama: X_train={len(X_train)}, y_train={len(y_train)}\"\n",
        "\n",
        "# 1. Konversi teks ke TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 2. Terapkan SMOTE hanya sekali\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_tfidf_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# 3. Pastikan hasil SMOTE tetap sinkron\n",
        "print(f\"Setelah SMOTE: X_train_tfidf_resampled={X_train_tfidf_resampled.shape}, y_train_resampled={y_train_resampled.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi-qqghkMRlb",
        "outputId": "8b3b6c4d-a4f5-48b4-9120-93d7f9740d62"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setelah SMOTE: X_train_tfidf_resampled=(6369, 3741), y_train_resampled=(6369,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train SVM Model"
      ],
      "metadata": {
        "id": "MRPszdpgUPRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='linear', C=1.5)\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGDz55r5URoY",
        "outputId": "364a9686-ed61-467a-89b8-a6e92f053d32"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9915110356536503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.86      0.92        28\n",
            "     neutral       0.99      1.00      1.00       531\n",
            "    positive       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.99       589\n",
            "   macro avg       1.00      0.94      0.97       589\n",
            "weighted avg       0.99      0.99      0.99       589\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Word2Vec Model"
      ],
      "metadata": {
        "id": "uNL4oEi7Ue8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [text.split() for text in X_train]\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "X_train_w2v = np.array([np.mean([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv] or [np.zeros(100)], axis=0) for text in X_train])\n",
        "X_test_w2v = np.array([np.mean([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv] or [np.zeros(100)], axis=0) for text in X_test])"
      ],
      "metadata": {
        "id": "4SSbfLHBUil0"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Random Forest Model"
      ],
      "metadata": {
        "id": "N5Hyb61DUnkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train_w2v, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test_w2v)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGIGQ29CUp3O",
        "outputId": "fac60423-e8f5-41ef-8968-5138d662cb12"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9490662139219015\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.04      0.07        28\n",
            "     neutral       0.95      1.00      0.97       531\n",
            "    positive       1.00      0.90      0.95        30\n",
            "\n",
            "    accuracy                           0.95       589\n",
            "   macro avg       0.98      0.65      0.66       589\n",
            "weighted avg       0.95      0.95      0.93       589\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer for LSTM"
      ],
      "metadata": {
        "id": "ibm8d7zkY8iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=100)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=100)\n"
      ],
      "metadata": {
        "id": "8sKkLj95Y9bd"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare embedding matrix"
      ],
      "metadata": {
        "id": "Ky7VqZrMY_-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((5000, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < 5000 and word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]"
      ],
      "metadata": {
        "id": "scPpsdRbZCxd"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode labels"
      ],
      "metadata": {
        "id": "q_HmTH2gZE_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded = y_train.map({\"negative\": 0, \"neutral\": 1, \"positive\": 2})\n",
        "y_test_encoded = y_test.map({\"negative\": 0, \"neutral\": 1, \"positive\": 2})"
      ],
      "metadata": {
        "id": "sXbI_XskZIIQ"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert labels to categorical"
      ],
      "metadata": {
        "id": "gOXp98hGZKsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded = to_categorical(y_train_encoded, num_classes=3)\n",
        "y_test_encoded = to_categorical(y_test_encoded, num_classes=3)"
      ],
      "metadata": {
        "id": "FduiBkTcZNNh"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build LSTM Model"
      ],
      "metadata": {
        "id": "HYOzry4jZRU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = Sequential([\n",
        "    Embedding(5000, 100, weights=[embedding_matrix], input_length=100, trainable=False),\n",
        "    SpatialDropout1D(0.2),\n",
        "    Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq1xuufZZSOS",
        "outputId": "665c1d6d-4157-47b5-91d4-873e4fb0db4a"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train LSTM Model"
      ],
      "metadata": {
        "id": "ZSsf5wa6ZVEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.fit(X_train_seq, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test_seq, y_test_encoded))\n",
        "\n",
        "y_pred_lstm = np.argmax(lstm_model.predict(X_test_seq), axis=1)\n",
        "y_test_labels = np.argmax(y_test_encoded, axis=1)\n",
        "print(\"LSTM Accuracy:\", accuracy_score(y_test_labels, y_pred_lstm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzo2WDO0ZXVt",
        "outputId": "3da329b2-3822-40d9-bd52-98ec2eea26c5"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 558ms/step - accuracy: 0.8551 - loss: 0.6156 - val_accuracy: 0.9015 - val_loss: 0.3484\n",
            "Epoch 2/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 523ms/step - accuracy: 0.9074 - loss: 0.3479 - val_accuracy: 0.9015 - val_loss: 0.3418\n",
            "Epoch 3/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 508ms/step - accuracy: 0.9019 - loss: 0.3511 - val_accuracy: 0.9015 - val_loss: 0.3409\n",
            "Epoch 4/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 486ms/step - accuracy: 0.8966 - loss: 0.3582 - val_accuracy: 0.9015 - val_loss: 0.3319\n",
            "Epoch 5/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 561ms/step - accuracy: 0.9064 - loss: 0.3400 - val_accuracy: 0.9015 - val_loss: 0.3367\n",
            "Epoch 6/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 509ms/step - accuracy: 0.9008 - loss: 0.3559 - val_accuracy: 0.9015 - val_loss: 0.3315\n",
            "Epoch 7/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 523ms/step - accuracy: 0.9031 - loss: 0.3482 - val_accuracy: 0.9015 - val_loss: 0.3296\n",
            "Epoch 8/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 490ms/step - accuracy: 0.8932 - loss: 0.3689 - val_accuracy: 0.9015 - val_loss: 0.3333\n",
            "Epoch 9/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 494ms/step - accuracy: 0.9038 - loss: 0.3390 - val_accuracy: 0.9015 - val_loss: 0.3270\n",
            "Epoch 10/10\n",
            "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 535ms/step - accuracy: 0.9029 - loss: 0.3449 - val_accuracy: 0.9015 - val_loss: 0.3240\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step\n",
            "LSTM Accuracy: 0.9015280135823429\n"
          ]
        }
      ]
    }
  ]
}